\section{Uncertainty Quantification in the Economic Literature}
\thispagestyle{plain} % surpress header on first page

The need for UQ as an essential part of quantitative economic studies has long been recognized in the economics profession.\footnote{See \cite{Hansen.1996}, \cite{Kydland.1992} and \cite{Canova.1994}, amongst others.} Also GSA in particular has had strong advocates.\footnote{See \cite{Canova.1995} and \cite{Gregory.1995}.} However, the demanded evolution of research practice has only been met by a few publications until today. This literature review summarizes these publications with regards to two UQ subfields that are emphasized in the prior section. These are uncertainty analysis and quantitative GSA. The review excludes qualitative GSAs because factor fixing is not the objective of the respective publications. A qualitative GSA as a standalone GSA is not considered as best practice (\cite{Saltelli.2004})\footnote{See p. 48.}. Table \ref{tab:lit} gives an overview of the major topics, analyses, measures and methods in the literature.\\

\noindent
I find 14 contributions that meet the described criteria. Arguably, because UQ is more accomplished in climatology, a large share of research comes from climate economics. Another field where UQ finds some application is macroeconomics. Remarkably, no contribution computes their own estimates for the model input uncertainty. The earlier publications tend to use the conceptually simple Monte Carlo uncertainty analysis. However, some prefer Latin hypercube sampling. The idea of Latin hypercube sampling is to improve the speed with which the random draws cover the whole variable range. For this purpose, the range is divided into equally probable intervals. Then, one draws only once from each possible interval combination by discarding further draws of the same combinations. The later contributions focus on GSA. \cite{Harenberg.2019} gives a well-argued explanation about why GSAs are better than LSAs. GSA measures are Sobol' indices, univariate effects and two density-based measures. The majority of papers use surrogate models to save computation time. The recent works use more sophisticated methods like polynomial chaos expansions to construct a surrogate model (as first applied in \cite{Harenberg.2019}) or intrusive approaches (see, for instance, \cite{Scheidegger.2019}). Intrusive methods require essential changes to the model structure, for instance to the state space, whereas the usual non-intrusive methods leave the model untouched and treat it like a so-called black box.\\
\newline
\cite{Harrison.1992} suggest to use uncertainty analysis via Monte Carlo sampling for applied general equilibrium modeling to inspect the uncertainty in model inputs. As a showcase, they propagate the distributions of 48 elasticities through a taxation model by drawing 15,000 input parameter vectors. They analyse their results graphically, using a histogram for their QoI as well as confidence intervals for its mean. For further use, N denotes the size of a Monte Carlo sample.\\
\newline
\cite{Canova.1994} proposes to perform a Monte Carlo uncertainty analysis to reflect upon the calibration of dynamic general equilibrium models. The author also addresses challenges and methods for parameter calibration. \citeauthor{Canova.1994} illustrates his approach by plotting distributions and computing moments and prediction intervals for QoIs in an asset-pricing (N=10,000) and a real business cycle model (N=1,000). Moreover, he analyzes the QoIs' sensitivity towards the uncertainty of individual input parameters by propagating different specifications of input distributions.\\
\newline
More recent examples for Monte Carlo uncertainty analysis investigate climate models, such as \cite{Webster.2012}.
Examples using Latin hypercube sampling are \cite{Mattoo.2009} and \cite{Hope.2006}.\\
\newline
Recently, \cite{Harenberg.2019} compare measures from LSA to measures from GSA for multiple QoIs of the canonical, macroeconomic real business cycle model. Thereby, they provide a context for GSA within UQ. The computed sensitivity measures are Sobol' indices and univariate effects. Univariate effects are the conditional expectation of a QoI as a function of one input parameter $X_i$, where the expectations are taken over $X_{\sim i}$. They are equal to the argument in the variance numerator of the first-order Sobol' index in Equation (\ref{eq:spec_sobol}). The sensitivity indices and univariate effects are obtained by polynomial chaos expansions. For this purpose, \citeauthor{Harenberg.2019} introduce the leave-one-out error estimator as a measure to select an orthogonal polynomial as the surrogate model.

The concept behind this estimator is the following: take an arbitrary set $A$ of a large number of $n$ input parameter vectors. From this set, create a set $B$ of $n$ sets that contains every possible permutation of set $A$ but leaving out one parameter vector. Then, for each candidate surrogate model specification, first, compute $n$ surrogate models by evaluating each element of set $B$. Second, for each specification, compute the mean of the squared errors between actual and surrogate model evaluated at each element of $B$. This is the leave-one-out error. Finally, one chooses a surrogate model (computed from an arbitrary element of $B$) for the specification with the lowest error.

The authors come to the following conclusion: on the one hand, a LSA can easily be misleading due to the reasons detailed in the previous chapter. LSA methods are typically used in economics. The authors conclude that these are neither adequate for identifying the inputs that drive the uncertainty, nor do they allow to analyse interactions. On the other hand, a GSA can provide profound insights, and polynomial chaos expansions are a fast way to compute approximations for the respective global sensitivity measures.\\
\newline
\cite{Ratto.2008} presents global sensitivity measures for multiple variants of DSGE models computed by Monte Carlo methods and surrogate models. The first measure is density based and derived from the Smirnov test (see, e.g., \cite{Hornberger.1981}): the QoI range is partitioned into a desired set $S$, and an undesired set $\overline{S}$. Then a Monte Carlo sample of parameter vectors from the input distribution is propagated through the model. From the QoI realizations for each set, two cumulative distribution functions for each input parameter, one conditioned on QoI realizations in set $S$, and the other conditioned on realizations in set $\overline{S}$, are generated. For each input independently, it is tested whether the distributions differ. If they do, the parameters and their specific regions that lead to the undesired QoI realizations can directly be identified. The second measure is first-order Sobol' indices. \citeauthor{Ratto.2008} computes them by employing two different surrogate models. The first surrogate is obtained by state-dependent regression. The idea is to regress the QoI on (combinations of) input parameters. The second surrogate is a polynomial representation of the first one. The author finds that the surrogates provide a good fit for the Monte Carlo sample except for the distribution tails. The fit varies conditional on different input parameters. \citeauthor{Ratto.2008} compares his results for the first-order Sobol' indices computed by both surrogates. The results show some differences in size but not in ranking.\\
\newline
\cite{Saltelli.2010} criticise the arbitrary input value choices in the sensitivity analysis design of the influential \cite{Stern.2007} report about the consequences of climate change. Particularly, \citeauthor{Stern.2007} argues that this cost-benefit analysis' results about the economic impact of climate change are robust towards the uncertainty in the input parameters. Yet, \cite{Saltelli.2010} contradict \citeauthor{Stern.2007}'s assertion by presenting a more thorough sensitivity analysis with parameter choices that better represent the original input distribution.\\
\newline
A series of papers (\cite{Anderson.2014}, \cite{Butler.2014}, \cite{Miftakhova.2018}) conducts sensitivity analyses for the dynamic integrated climate-economy model in \cite{Nordhaus.2008}, in short DICE model.  Each work concludes that a GSA is superior to a LSA. Furthermore, all contributions find that leaving some hypothetically low-impact parameters out of the sensitivity analyses lead \citeauthor{Nordhaus.2008} to neglect the uncertainty in important parameters.

\cite{Anderson.2014} use Sobol' indices, the $\delta$-sensitivity measure, and correlation measures for paired QoIs in their GSA. The $\delta$-sensitivity measure (see, e.g., \cite{Borgonovo.2006}) is density-based. It is given by half the expectation value of the absolute difference between the unconditional distribution of a QoI and the QoI distribution conditioned on one specific, fixed input (group). Estimates for these measures are computed with the algorithm used in \cite{Plischke.2013} applied to a Monte-Carlo sample (N=10,000). In \cite{Anderson.2014}, the $\delta$-sensitivity measure is the main measure of sensitivity and used to rank the parameters in terms of their contributions to the model uncertainty. The authors also use a surrogate model obtained through Cut-HDMR (cut-high dimensional model representation; see, e.g., \cite{Ziehn.2009}) for graphical analyses of the interactions between input parameters.

\cite{Butler.2014} also generate importance rankings for the uncertainty in input parameters. However, they use first, second and total order Sobol' indices instead of the $\delta$-sensitivity measure. They compute the Sobol' indices based on Sobol' sequences (\cite{Sobol.1967}) for the results and based on Latin Hypercube sampling (\cite{McKay.1979}) as a check. The results in \cite{Butler.2014} and \cite{Anderson.2014} can not be compared as they analyse different QoIs.

\cite{Miftakhova.2018} applies the GSA procedure outlined by \cite{Harenberg.2019}. The importance ranking that she obtains from the polynomial-chaos-expansions-based Sobol' indices is different from the ranking that \cite{Anderson.2014} obtain from the $\delta$-sensitivity measure. Yet, this is not mentioned by \citeauthor{Miftakhova.2018}.\footnote{I do not have access to the numerical codes. Thus the reasons for the discrepancies remain unclear.} However, the author emphasizes that the standard procedure for obtaining Sobol' indices from a variance decomposition as used by \cite{Anderson.2014} and \cite{Butler.2014} is not feasible for the DICE model because a set of input parameters is calibrated jointly in order to let the model match some observables. Therefore, although these input parameters are not correlated in the classical sense, they are dependent. Hence, the variance-based Sobol decomposition is not applicable because the summands are not orthogonal to each other or, in other words, the input-specific variance terms contain a covariance component. Thus, they do not add to the total model variance and Sobol' indices cannot be computed directly. \cite{Miftakhova.2018} shows how the set of dependent input parameters can be changed to a set of independent parameters by changing the model structure: she includes uncertain observables as independent parameters and reformulates dependent input parameters as endogenous variables. These endogenous variables are functions of the remaining, formerly dependent parameters and the new input parameters.\footnote{For a discussion of more general methods to compute Sobol' indices in the presence of dependent input parameters see, e.g., \cite{Chastaing.2015} and \cite{Wiederkehr.2018}, with references therein.}\\
\newline
\cite{Gillingham.2015} conduct an UQ for six major climate models. They select three input parameters that are present in each model. The authors generate a surrogate model from regressing several model outputs separately on a linear-quadratic-interaction specification of the three input parameters on 250 grid points. Then they draw 1,000,000 parameter vectors randomly from the probability density function of the input parameters and evaluate the sample with the surrogate model. They find that the input uncertainty contributes to more than 90\% whereas the differences in the six models contribute to less than 10\% of the QoI variances for the year 2100. They also present QoI values for multiple percentiles of each input parameter.\\
\newline
Most recently, \cite{Scheidegger.2019} made a noteworthy contribution that naturally connects the solution process of economic models to UQ with surrogate models. The difference to the prior contributions is that their method is intrusive instead of non-intrusive. In particular, they conduct an uncertainty analysis and compute univariate effects. \citeauthor{Scheidegger.2019}' approach is to solve very-high-dimensional dynamic programming problems by approximating and interpolating the value function with a combination of the active subspace method (see, e.g., \cite{Constantine.2015}) and Gaussian process regression (see, for example, \cite{Rasmussen.2005}) within each iteration of the value function iteration algorithm. The authors can apply their method up to a 500-dimensional stochastic growth model. Therefore, they can solve models that contain substantial parameter heterogeneity.
The link to UQ is that one can also "directly solve for all possible steady state policies as a function of the economic states and parameters in a single computation" \cite[p.~4]{Scheidegger.2019} from the value function interpolant. In other words, this step yields the QoI expressed by a surrogate model. Thus, to add an UQ, one has to, first, specify the uncertain parameters as continuous state variables, and second, assign a probability distribution to each of these parameters. Then (assuming the uncertain input parameters are independent), one provides a sample from each parameter's distribution as input to the Gaussian process regression to obtain a surrogate model. Following these steps, QoIs can be expressed as functions of the uncertain input parameters without much additional effort. Finally, by using a processed value function interpolant as a surrogate model, \citeauthor{Scheidegger.2019} propagate the model uncertainty and depict univariate effects.\\
\newline
Building on the contributions by \cite{Harenberg.2019} and \cite{Scheidegger.2019}, \cite{Usui.2019} conducts a GSA based on Sobol' indices and univariate effects to study rare natural disasters in a dynamic stochastic economy. Because the repeated model evaluations required to construct an adequate surrogate model are too computationally expensive, they choose to apply a  method similar to \citeauthor{Scheidegger.2019}' intrusive framework. However different to \cite{Scheidegger.2019}, they generate numerical approximates for their policy functions by time iteration collocation (see, e.g., \cite{Judd.1998}) with adaptive sparse grid (see  \cite{Scheidegger.2018}) instead of Gaussian machine-learning.\\
\newline
The thesis is distinct from most of the literature in the following points. First, it analyses a labour economic model with a larger number of parameters. Moreover, it uses its own estimates for the input uncertainty. These estimates imply correlated input parameters. Furthermore, the model evaluation is relatively costly. Therefore, this work computes screening measures for factor fixing to prepare a subsequent quantitative GSA.
\\
\newline
The next section describes the method in \cite{ge2014efficient} to compute EE-based measures for models with correlated input parameters. It also addresses important drawbacks of these measures and develops them further.