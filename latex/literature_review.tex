\section{Uncertainty Quantification in the Economic \\ Literature}
\thispagestyle{plain} % surpress header on first page

The need for UQ as an essential part of quantitative economic studies has long been recognized in the economics profession.\footnote{See \cite{Hansen.1996}, \cite{Kydland.1992} and \cite{Canova.1994}, amongst others.} Also GSA in particular has had strong advocates.\footnote{See \cite{Canova.1995} and \cite{Gregory.1995}.} However, the demanded evolution of research practice has only been met by a few publications until today. This literature review summarizes these publications with regards to the UQ subfields that are emphasized in the prior section. These are uncertainty propagation and GSA. Table \ref{tab:lit} gives an overview of the major topics, analyses, measures and methods in the literature.

\begin{table}[H]
	\centering
	\caption{Overview of UQ literature}
	\label{tab:lit}
	\renewcommand{\arraystretch}{1.2}%
	\begin{tabular}{lc}
		\toprule
		Content                      & Number of articles \\ \midrule
		$Topics$                       &                    \\
		\qquad Climate economics            & 8                  \\
		\qquad Macroeconomics               & 4                 \\ \midrule
		$Analyses$                     &                    \\
		\qquad Uncertainty propagation      & 8                  \\
		\qquad Globabl sensitivity analysis & 7                  \\
		\qquad Local sensitivity analysis   & 2                  \\ \midrule
		$Measures$                     &                    \\
		\qquad Sobol' indices               & 6                  \\
		\qquad Univariate effects           & 4                  \\
		\qquad Density-based measures & 2                  \\ \midrule
		$Methods$                      &                    \\
		\qquad Monte Carlo sampling         & 7                  \\
		\qquad Latin hypercube sampling         & 3                  \\
		\qquad Surrogate model              & 7                  \\
		\qquad Polynomial chaos expansions  & 2                  \\
		\qquad Intrusive methods            & 2                  \\ \midrule
		& 14                 \\ \bottomrule
	\end{tabular}
\end{table}
\noindent
I find 14 contributions that meet the described criteria. Arguably, because UQ is more accomplished in climatology, a large share of research comes from climate economics. Another field where UQ finds some application is macroeconomics. Remarkably, no contribution computes their own estimates for the parametric model uncertainty. The earlier publications tend to use the conceptually simple Monte Carlo uncertainty propagation. However, some prefer Latin hypercube sampling. The idea of Latin hypercube sampling is to improve the speed with which the random draws cover the whole variable range. For this purpose, the range is divided into equally probable intervals. Then, one draws only once from each possible interval combination by discarding further draws of the same combinations. The later contributions focus on GSA. \cite{Harenberg.2019} gives a well-argued explanation about why GSAs are better than LSAs. GSA measures are Sobol' indices, univariate effects and two density-based measures. The majority of papers use surrogate models to save computation time. The recent works use more sophisticated methods like polynomial chaos expansions (as first applied in \cite{Harenberg.2019}) or intrusive approaches (see, for instance, \cite{Scheidegger.2019}). This section concludes by explaining the choice of measures and methods made in this thesis and by comparing them to those used in the literature.\\
\newline
\cite{Harrison.1992} suggest to use uncertainty propagation via Monte Carlo sampling for applied general equilibrium modeling to inspect the uncertainty in model inputs. As a showcase, they propagate the distributions of 48 elasticities through a taxation model by drawing 15,000 input parameter vectors. They analyse their results graphically, using a histogram for their QoI as well as confidence intervals for its mean. For further use, N denotes the size of a Monte Carlo sample.\\
\newline
\cite{Canova.1994} proposes to perform a Monte Carlo uncertainty propagation to reflect upon the calibration of dynamic general equilibrium models. The author also addresses challenges and methods for parameter calibration. \citeauthor{Canova.1994} illustrates his approach by plotting distributions and computing moments and prediction intervals for QoIs in an asset-pricing (N=10,000) and a real business cycle model (N=1,000). Moreover, he analyzes the QoIs' sensitivity towards the uncertainty of individual input parameters by propagating different specifications of input distributions.\\
\newline
More recent examples for Monte Carlo uncertainty propagation investigate climate models, such as \cite{Webster.2012}.
Examples using Latin hypercube sampling are \cite{Mattoo.2009} and \cite{Hope.2006}.\\
\newline
Recently, \cite{Harenberg.2019} compare measures from LSA to measures from GSA for multiple QoIs of the canonical, macroeconomic real business cycle model. Thereby, they provide a context for GSA within UQ. The computed sensitivity measures are Sobol' indices and univariate effects. They are obtained by polynomial chaos expansions. For this purpose, \citeauthor{Harenberg.2019} introduce the leave-one-out error estimator as a measure to select an orthogonal polynomial as the surrogate model.

The concept behind this estimator is the following: Take an arbitrary set $A$ of a large number of $n$ input parameter vectors. From this set, create a set $B$ of $n$ sets that contains every possible permutation of set $A$ but leaving out one parameter vector. Then, for each candidate surrogate model specification, first, compute $n$ surrogate models by evaluating each element of set $B$. Second, for each specification, compute the mean of the squared errors between actual and surrogate model evaluated at each element of $B$. This is the leave-one-out error. Finally, one chooses a surrogate model (computed from an arbitrary element of $B$) for the specification with the lowest error.

The authors come to the following conclusion: On the one hand, a LSA can easily be misleading because its perspective is not broad enough. In particular, they criticise the one-at-a-time approach on which LSAs rely. One-at-a-time methods base on changing one uncertain parameter while keeping the others constant. The choice of parameter combinations tends to be arbitrary. These methods are typically used in economics. The authors conclude that LSA is neither adequate for identifying the inputs that drive the uncertainty, nor does it allow to analyse interactions. On the other hand, a GSA can provide profound insights, and polynomial chaos expansions are a fast way to compute approximations for the respective global sensitivity measures.\\
\newline
\cite{Ratto.2008} presents global sensitivity measures for multiple variants of DSGE models computed by Monte Carlo methods and surrogate models. The first measure is density based and derived from the Smirnov test (see, e.g., \cite{Hornberger.1981}): The QoI range is partitioned into a desired set $S$, and an undesired set $\overline{S}$. Then a Monte Carlo sample of parameter vectors from the input distribution is propagated through the model. From the QoI realizations for each set, two cumulative distribution functions for each input parameter, one conditioned on QoI realizations in set $S$, and the other conditioned on realizations in set $\overline{S}$, are generated. For each input independently, it is tested whether the distributions differ. If they do, the parameters and their specific regions that lead to the undesired QoI realizations can directly be identified. The second measure is first-order Sobol' indices. \citeauthor{Ratto.2008} computes them by employing two different surrogate models. The first surrogate is obtained by state-dependent regression. The idea is to regress the QoI on (combinations of) input parameters. The second surrogate is a polynomial representation of the first one. The author finds that the surrogates provide a good fit for the Monte Carlo sample except for the distribution tails. The fit varies conditional on different input parameters. \citeauthor{Ratto.2008} compares his results for the first-order Sobol' indices computed by both surrogates. The results show some differences in size but not in ranking.\\
\newline
\cite{Saltelli.2010} criticise the arbitrary input value choices in the sensitivity analysis design of the influential \cite{Stern.2007} report about the consequences of climate change. Particularly, \citeauthor{Stern.2007} argue that their cost-benefit analysis' results about the economic impact of climate change are robust towards the uncertainty in their input parameters. Yet, \cite{Saltelli.2010} contradict \citeauthor{Stern.2007}'s assertion by presenting a more thorough sensitivity analysis with parameter choices that better represent the original input distribution.\\
\newline
A series of papers (\cite{Anderson.2014}, \cite{Butler.2014}, \cite{Miftakhova.2018}) conducts sensitivity analyses for the Dynamic Integrated Climate-Economy model in \cite{Nordhaus.2008}, in short DICE model.  Each work concludes that a GSA is superior to a LSA for the same reasons as \cite{Harenberg.2019}. Furthermore, all contributions find that leaving some hypothetically low-impact parameters out of the sensitivity analyses lead \citeauthor{Nordhaus.2008} to neglect the uncertainty in important parameters.

\cite{Anderson.2014} use Sobol' indices, the $\delta$-sensitivity measure, and correlation measures for paired QoIs in their GSA. The $\delta$-sensitivity measure (see, e.g., \cite{Borgonovo.2006}) is density-based. It is given by half the expectation value of the absolute difference between the unconditional distribution of a QoI and the QoI distribution conditioned on one specific, fixed input (group). Estimates for these measures are computed with the algorithm used in \cite{Plischke.2013} applied to a Monte-Carlo sample (N=10,000). In \cite{Anderson.2014}, the $\delta$-sensitivity measure is the main measure of sensitivity and used to rank the parameters in terms of their contributions to the model uncertainty. The authors also use a surrogate model obtained through Cut-HDMR (Cut-High Dimensional Model Representation; see, e.g., \cite{Ziehn.2009}) for graphical analyses of the interaction between input parameters.

\cite{Butler.2014} also generate importance rankings for the uncertainty in input parameters. However, they use first, second and total order Sobol' indices instead of the $\delta$-sensitivity measure. They compute the Sobol' indices based on Sobol' sequences (\cite{Sobol.1967}) for the results and based on Latin Hypercube sampling (\cite{McKay.1979}) as a check. The results in \cite{Butler.2014} and \cite{Anderson.2014} can not be compared as they analyse different QoIs.

\cite{Miftakhova.2018} applies the GSA procedure outlined by \cite{Harenberg.2019}. The importance ranking that she obtains from the polynomial-chaos-expansions-based Sobol' indices is different from the ranking that \cite{Anderson.2014} obtain from the $\delta$-sensitivity measure. Yet, this is not mentioned by \citeauthor{Miftakhova.2018}.\footnote{I do not have access to the numerical codes. Thus the reasons for the discrepancies remain unclear.} However, the author emphasizes that the standard procedure for obtaining Sobol' indices from a variance decomposition as used by \cite{Anderson.2014} and \cite{Butler.2014} is not feasible for the DICE model because a set of input parameters is calibrated jointly in order to let the model match some observables. Therefore, although these input parameters are not correlated in the classical sense, they are dependent. Hence, the variance-based Sobol decomposition is not applicable because the summands are not orthogonal to each other or, in other words, the input-specific variance terms contain a covariance component. Thus, they do not add to the total model variance and Sobol' indices cannot be computed directly. \cite{Miftakhova.2018} shows how the set of dependent input parameters can be changed to a set of independent parameters by changing the model structure: She includes uncertain observables as independent parameters and reformulates dependent input parameters as endogenous variables. These endogenous variables are functions of the remaining, formerly dependent parameters and the new input parameters.\footnote{For a discussion of more general methods to compute Sobol' indices in the presence of dependent input parameters see, e.g., \cite{Chastaing.2015} and \cite{Wiederkehr.2018}, with references therein.}\\
\newline
\cite{Gillingham.2015} conduct an UQ for six major climate models. They select three input parameters that are present in each model. The authors generate a surrogate model from regressing several model outputs separately on a linear-quadratic-interaction specification of the three input parameters on 250 grid points. Then they draw 1,000,000 parameter vectors randomly from the probability density function of the input parameters and evaluate the sample with the surrogate model. They find that the parametric uncertainty contributes to more than 90\% whereas the differences in the six models contribute to less than 10\% of the QoI variances for the year 2100. They also present QoI values for multiple percentiles of each input parameter.\\
\newline
Most recently, \cite{Scheidegger.2019} made a noteworthy contribution that naturally connects the solution process of economic models to UQ with surrogate models. The difference to the prior contributions is that their method is intrusive instead of non-intrusive (see page XX). In particular, they conduct an uncertainty propagation and compute univariate effects. \citeauthor{Scheidegger.2019}' approach is to solve very-high-dimensional dynamic programming problems by approximating and interpolating the value function with a combination of the active subspace method (see, e.g., \cite{Constantine.2015}) and Gaussian process regression (see, for example, \cite{Rasmussen.2005}) within each iteration of the value function iteration algorithm. The authors can apply their method up to a 500-dimensional stochastic growth model. Therefore, they can solve models that contain substantial parameter heterogeneity.
The link to UQ is that one can also "directly solve for all possible steady state policies as a function of the economic states and parameters in a single computation" \cite[p.~4]{Scheidegger.2019} from the value function interpolant. In other words, this step yields the QoI expressed by a surrogate model. Thus, to add an UQ, one has to, first, specify the uncertain parameters as continuous state variables, and second, assign a probability distribution to each of these parameters. Then (assuming the uncertain input parameters are independent), one provides a sample from each parameter's distribution as input to the Gaussian process regression to obtain a surrogate model. Following these steps, QoIs can be expressed as functions of the uncertain input parameters without much additional effort. Finally, by using a processed value function interpolant as a surrogate model, \citeauthor{Scheidegger.2019} propagate the model uncertainty and depict univariate effects.\\
\newline
Building on the contributions by \cite{Harenberg.2019} and \cite{Scheidegger.2019}, \cite{Usui.2019} conducts a GSA based on Sobol' indices and univariate effects to study rare natural disasters in a dynamic stochastic economy. Because the repeated model evaluations required to construct an adequate surrogate model are too computationally expensive, they choose to apply a  method similar to \citeauthor{Scheidegger.2019}' intrusive framework. However different to \cite{Scheidegger.2019}, they generate numerical approximates for their policy functions by time iteration collocation (see, e.g., \cite{Judd.1998}) with adaptive sparse grid (see  \cite{Scheidegger.2018}) instead of Gaussian machine-learning.\\
\newline
The remaining section explains where the thesis places within the literature and what it adds to it.

The first contribution is that it quantifies the uncertainty of a model in a merely microeconomic field, namely the model of occupational choice in \cite{Keane.1994}.

The second contribution is that the estimates for the parametric uncertainty are computed in the same contribution. This adds an important layer of transparency to the uncertainty quantification.

In line with the literature, the thesis' objective is to quantify the parametric uncertainty in an economic model and to attribute shares of this uncertainty to individual input parameters by means of a GSA. For this purpose, I carry out an uncertainty propagation to get an overview of the QoI's probability distribution, given the joint uncertainty in all input parameters. It is shown that this distribution is bell-shaped. Therefore, the variance-based Sobol' indices are a suitable GSA measure. The density-based measures are discarded because they are tailored to less simple distributions.

The specific methods are derived from the following three properties of the computational model. These are: First, the input parameters are correlated. Second, 27 input parameters is a moderately large number. Third, the computation time of the computational model is not rapidly short. The second point mainly amplifies the third point. The combination of these properties is also a novelty in the economic literature. Therefore, the approach that is explained in the following is another contribution to this literature.

There are multiple ways to deal with the moderate computational costs and the parameter dependencies. Each has its advantages and disadvantages. However, the general approach of this thesis is to circumvent unnecessary degrees of methodical complexity to avoid errors. Thus, I apply the following methods: First, I decrease the number of uncertain input parameters through Morris screening as part of an intermediate GSA. This approach is chosen over the alternative of constructing a surrogate model.
Second, a quasi-Monte Carlo scheme is used to compute the Sobol' indices in the presence of parameter dependencies. The discarded alternatives are the decorrelation techniques Rosenblatt and Nataf transformations.\\
\newline
In summary, the thesis makes the following contributions. First, it quantifies the parametric uncertainty for a merely microeconomic model. Second, it computes own estimates for this uncertainty. The last contributions emphasize that the thesis is the first contribution that carries out a GSA for a model with a moderate number of correlated input parameters: Third, it reduces the number of uncertain, correlated input parameters through Morris screening to prepare the next GSA step. Fourth, it computes Sobol' indices for correlated input parameters using a quasi-Monte Carlo scheme.\\
\newline
The next section describes the model in \cite{Keane.1994} and the estimation of the joint distribution of its input parameters.
